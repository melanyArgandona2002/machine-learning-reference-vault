{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true
      },
      "source": [
        "Jupyter Notebook: Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## 1. Importing Required Libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 2. Sigmoid function: Activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 3. Calculate activation for a neuron\n",
        "def calculate_activation(x, w, b):\n",
        "    \"\"\"\n",
        "    Calculates the activation of a neuron.\n",
        "    x: input vector\n",
        "    w: neuron weights\n",
        "    b: neuron bias\n",
        "    \"\"\"\n",
        "    z = np.dot(w, x) + b\n",
        "    a = sigmoid(z)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 4. Example of calculation in a layer\n",
        "def hidden_layer(x, w, b):\n",
        "    \"\"\"\n",
        "    Calculates the output of a hidden layer.\n",
        "    x: inputs from the previous layer\n",
        "    w: layer weights\n",
        "    b: layer biases\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    for i in range(w.shape[0]):\n",
        "        activations.append(calculate_activation(x, w[i], b[i]))\n",
        "    return np.array(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 5. Define layers and weights for the neural network\n",
        "def initialize_neural_network(input_size, hidden_layer_size, output_size):\n",
        "    \"\"\"\n",
        "    Initializes weights and biases for a simple neural network.\n",
        "    input_size: size of the input layer\n",
        "    hidden_layer_size: size of the hidden layer\n",
        "    output_size: size of the output layer\n",
        "    \"\"\"\n",
        "    # Weights and biases for the hidden layer\n",
        "    W1 = np.random.randn(hidden_layer_size, input_size) * 0.1\n",
        "    b1 = np.zeros((hidden_layer_size, 1))\n",
        "    \n",
        "    # Weights and biases for the output layer\n",
        "    W2 = np.random.randn(output_size, hidden_layer_size) * 0.1\n",
        "    b2 = np.zeros((output_size, 1))\n",
        "    \n",
        "    return W1, b1, W2, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 6. Forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    Performs forward propagation through the neural network.\n",
        "    X: inputs to the network\n",
        "    W1, b1: weights and biases of the hidden layer\n",
        "    W2, b2: weights and biases of the output layer\n",
        "    \"\"\"\n",
        "    # Hidden layer\n",
        "    Z1 = np.dot(W1, X.T) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    \n",
        "    # Output layer\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    return A2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 7. Example of prediction: Neural network for binary classification\n",
        "def prediction(A2):\n",
        "    \"\"\"\n",
        "    Makes a prediction for binary classification.\n",
        "    A2: activation of the output layer\n",
        "    \"\"\"\n",
        "    return (A2 >= 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 8. Visualizing the neural network structure\n",
        "def visualize_neural_network():\n",
        "    \"\"\"\n",
        "    Visualizes the structure of a simple neural network.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Layers of the neural network\n",
        "    ax.scatter([0]*4, np.linspace(1, 4, 4), s=500, c='r', label=\"Input Layer\")\n",
        "    ax.scatter([2]*3, np.linspace(1, 3, 3), s=500, c='g', label=\"Hidden Layer\")\n",
        "    ax.scatter([4]*1, [2], s=500, c='b', label=\"Output Layer\")\n",
        "    \n",
        "    # Connections between layers\n",
        "    for i in range(4):\n",
        "        for j in range(3):\n",
        "            ax.plot([0, 2], [i+1, j+1], c='k', lw=0.5)\n",
        "    for j in range(3):\n",
        "        ax.plot([2, 4], [j+1, 2], c='k', lw=0.5)\n",
        "    \n",
        "    # Add labels and legend\n",
        "    ax.set_title(\"Structure of a Simple Neural Network\")\n",
        "    ax.set_xticks([0, 2, 4])\n",
        "    ax.set_xticklabels([\"Input\", \"Hidden\", \"Output\"])\n",
        "    ax.set_yticks([1, 2, 3, 4])\n",
        "    ax.set_yticklabels([\"\", \"\", \"\", \"\"])\n",
        "    ax.legend(loc=\"upper right\")\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 9. Example prediction with the neural network\n",
        "input_size = 4  # Number of input features\n",
        "hidden_layer_size = 3  # Number of neurons in the hidden layer\n",
        "output_size = 1  # Number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize weights and biases\n",
        "W1, b1, W2, b2 = initialize_neural_network(input_size, hidden_layer_size, output_size)\n",
        "\n",
        "# Example data (features of the shirt)\n",
        "X = np.array([[10, 5, 20, 15]])  # Example: price, expenses, marketing, quality\n",
        "\n",
        "# Forward propagation\n",
        "A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "# Prediction\n",
        "prediction_result = prediction(A2)\n",
        "print(\"Prediction (0: Not a success, 1: Will be a success):\", prediction_result)\n",
        "\n",
        "# Visualize the neural network\n",
        "visualize_neural_network()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5: TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "### Example: Coffee Roasting\n",
        "# Simulating a simple binary classification example.\n",
        "# Features: Temperature and Duration.\n",
        "\n",
        "# Input data (Temperature, Duration)\n",
        "X = np.array([\n",
        "    [190, 12],  # Bad roast\n",
        "    [200, 15],  # Good roast\n",
        "    [210, 18],  # Bad roast\n",
        "    [205, 17]   # Good roast\n",
        "], dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Labels (0 = Bad flavor, 1 = Good flavor)\n",
        "Y = np.array([0, 1, 0, 1], dtype=float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "X = X / np.max(X, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Model Construction\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid', input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X, Y, epochs=500, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Infer with new data\n",
        "X_new = np.array([[200, 17], [210, 20]]) / np.max(X, axis=0)\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of matrices with NumPy\n",
        "x_numpy = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"NumPy Matrix:\")\n",
        "print(x_numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to TensorFlow\n",
        "x_tensor = tf.constant(x_numpy)\n",
        "print(\"TensorFlow Tensor:\")\n",
        "print(x_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert TensorFlow to NumPy\n",
        "x_back_to_numpy = x_tensor.numpy()\n",
        "print(\"From TensorFlow to NumPy:\")\n",
        "print(x_back_to_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of a simple neural network for digit classification\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model_digits = Sequential([\n",
        "    Dense(25, activation='sigmoid', input_shape=(784,)),  # Input of 784 pixels (28x28)\n",
        "    Dense(10, activation='softmax')  # Output with 10 classes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_digits.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example data (MNIST)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model_digits.fit(X_train, y_train, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluation = model_digits.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", evaluation[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Implementation with Matrix Multiplication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Example of Forward Propagation in a Single Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input (X), Weights (W), and Biases (B)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])  # 3x2 input matrix\n",
        "W = np.array([[0.1, 0.2], [0.3, 0.4]])  # 2x2 weights matrix\n",
        "B = np.array([[0.5, -0.5]])  # 1x2 biases matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward Propagation: Z = X @ W + B\n",
        "Z = np.matmul(X, W) + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply sigmoid activation function: A = sigmoid(Z)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = sigmoid(Z)\n",
        "print(\"Forward Propagation Output (A):\\n\", A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Matrix Multiplication Concept\n",
        "- Matrix multiplication can be thought of as a series of dot products.\n",
        "- For vectors, dot product is calculated as sum of element-wise products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dot product example\n",
        "v1 = np.array([1, 2])\n",
        "v2 = np.array([3, 4])\n",
        "\n",
        "dot_product = np.dot(v1, v2)\n",
        "print(\"Dot Product of v1 and v2:\", dot_product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix-vector multiplication\n",
        "M = np.array([[3, 4], [5, 6]])  # 2x2 matrix\n",
        "v = np.array([1, 2])  # 2x1 vector\n",
        "\n",
        "result = np.matmul(M, v)\n",
        "print(\"Matrix-Vector Multiplication Result:\\n\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Matrix Multiplication with Two Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A 2x3 matrix multiplied by a 3x4 matrix results in a 2x4 matrix.\n",
        "\n",
        "A = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3 matrix\n",
        "W = np.array([[7, 8, 9, 10], [11, 12, 13, 14], [15, 16, 17, 18]])  # 3x4 matrix\n",
        "\n",
        "# Matrix multiplication: Z = A @ W\n",
        "Z = np.matmul(A, W)\n",
        "print(\"Matrix-Matrix Multiplication Result:\\n\", Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transposition in Matrix Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transposing a matrix involves swapping rows with columns.\n",
        "\n",
        "A_T = A.T  # Transpose of matrix A\n",
        "print(\"Transpose of Matrix A:\\n\", A_T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sigmoid Activation and Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sigmoid_Z = sigmoid(Z)\n",
        "print(\"Sigmoid Output:\\n\", sigmoid_Z)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
